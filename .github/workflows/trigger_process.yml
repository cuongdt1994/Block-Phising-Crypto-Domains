name: Scheduled Script Execution

on:
  workflow_dispatch:
  schedule:
    - cron: "*/5 * * * *"

jobs:
  download-and-process:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Download and process all sources
        run: |
          cat << 'EOF' > process_domains.py
          import os
          import json
          import requests
          import re
          from urllib.parse import urlparse

          # Cloudflare DNS IPs to exclude
          EXCLUDED_IPS = {'1.1.1.1', '1.0.0.1'}

          # Domains to exclude
          EXCLUDED_DOMAINS = {
              'raw.githubusercontent.com',
              'gist.githubusercontent.com',
              's3.us-east-2.amazonaws.com',
              'cdn.discordapp.com'
          }

          def is_valid_domain(domain):
              """Validate domain format"""
              domain_pattern = r'^(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)*[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?$'
              return bool(re.match(domain_pattern, domain))

          def is_valid_ip(ip):
              """Validate IP format"""
              try:
                  parts = ip.split('.')
                  if len(parts) != 4:
                      return False
                  for part in parts:
                      if not part.isdigit():
                          return False
                      num = int(part)
                      if num < 0 or num > 255:
                          return False
                  return True
              except:
                  return False

          def is_valid_url(url):
              """Validate URL format"""
              try:
                  result = urlparse(url)
                  return all([result.scheme, result.netloc])
              except:
                  return False

          def extract_domain_from_url(url):
              """Extract domain from URL"""
              try:
                  parsed = urlparse(url)
                  return parsed.netloc
              except:
                  return None

          def process_ipsum_file(url):
              """Process IPsum file and extract IP addresses"""
              ips = []
              try:
                  print(f"Downloading from: {url}")
                  response = requests.get(url, timeout=30)
                  if response.status_code == 200:
                      lines = response.text.strip().split('\n')
                      for line in lines:
                          line = line.strip()
                          if line and not line.startswith('#') and not line.startswith('//'):
                              # IPsum files contain one IP per line
                              if is_valid_ip(line) and line not in EXCLUDED_IPS:
                                  ips.append(line)
                  else:
                      print(f"Failed to download {url}: HTTP {response.status_code}")
              except Exception as e:
                  print(f"Error processing {url}: {e}")
              
              return ips

          # List of IPsum sources (levels 1-8)
          ipsum_sources = [
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/1.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/2.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/3.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/4.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/5.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/6.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/7.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/8.txt'
          ]

          # Create directories
          os.makedirs('dist', exist_ok=True)

          # Initialize sets for different types of indicators
          domains = set()
          ips = set()
          urls = set()

          # Download and process JSON sources for domains
          print("=== PROCESSING JSON SOURCES FOR DOMAINS ===")
          
          # Download scamsniffer domains
          try:
              print("Processing scamsniffer domains...")
              scamsniffer_url = 'https://raw.githubusercontent.com/scamsniffer/scam-database/refs/heads/main/blacklist/domains.json'
              response = requests.get(scamsniffer_url, timeout=30)
              if response.status_code == 200:
                  scamsniffer_data = response.json()
                  for domain in scamsniffer_data:
                      if domain and domain not in EXCLUDED_DOMAINS and is_valid_domain(domain):
                          domains.add(domain)
                  print(f"Added {len([d for d in scamsniffer_data if d not in EXCLUDED_DOMAINS])} domains from scamsniffer")
              else:
                  print(f"Failed to download scamsniffer: HTTP {response.status_code}")
          except Exception as e:
              print(f"Error downloading scamsniffer: {e}")

          # Download metamask blacklist
          try:
              print("Processing metamask blacklist...")
              metamask_url = 'https://raw.githubusercontent.com/MetaMask/eth-phishing-detect/refs/heads/main/src/config.json'
              response = requests.get(metamask_url, timeout=30)
              if response.status_code == 200:
                  metamask_data = response.json()
                  blacklist_urls = metamask_data.get('blacklist', [])
                  for item in blacklist_urls:
                      if item:
                          item = item.strip()
                          if is_valid_url(item):
                              urls.add(item)
                              # Extract domain from URL
                              domain = extract_domain_from_url(item)
                              if domain and is_valid_domain(domain) and domain not in EXCLUDED_DOMAINS:
                                  domains.add(domain)
                          elif is_valid_domain(item) and item not in EXCLUDED_DOMAINS:
                              domains.add(item)
                  print(f"Added {len(blacklist_urls)} items from metamask")
              else:
                  print(f"Failed to download metamask: HTTP {response.status_code}")
          except Exception as e:
              print(f"Error downloading metamask: {e}")

          # Process IPsum sources for IPs
          print("=== PROCESSING IPSUM SOURCES FOR IPs ===")
          total_ipsum_ips = 0
          for i, source in enumerate(ipsum_sources):
              level = i + 1
              print(f"Processing IPsum level {level}...")
              try:
                  level_ips = process_ipsum_file(source)
                  ips.update(level_ips)
                  total_ipsum_ips += len(level_ips)
                  print(f"Added {len(level_ips)} IPs from level {level}")
              except Exception as e:
                  print(f"Error processing level {level}: {e}")

          # Write output files to dist directory
          print("=== WRITING OUTPUT FILES ===")
          
          # Write domains
          domains_list = sorted(list(domains))
          with open('dist/domains.txt', 'w') as f:
              for domain in domains_list:
                  f.write(domain + '\n')
          print(f"Written {len(domains_list)} domains to domains.txt")

          # Write IPs
          ips_list = sorted(list(ips))
          with open('dist/ips.txt', 'w') as f:
              for ip in ips_list:
                  f.write(ip + '\n')
          print(f"Written {len(ips_list)} IPs to ips.txt")

          # Write URLs
          urls_list = sorted(list(urls))
          with open('dist/urls.txt', 'w') as f:
              for url in urls_list:
                  f.write(url + '\n')
          print(f"Written {len(urls_list)} URLs to urls.txt")

          print(f"=== PROCESSING COMPLETE ===")
          print(f"Total Domains: {len(domains_list)}")
          print(f"Total IPs: {len(ips_list)} (from IPsum: {total_ipsum_ips})")
          print(f"Total URLs: {len(urls_list)}")
          print(f"Excluded domains: {EXCLUDED_DOMAINS}")
          print(f"Excluded IPs: {EXCLUDED_IPS}")

          EOF
          python process_domains.py

      - name: Move output files to lists directory
        run: |
          mkdir -p lists
          mv dist/domains.txt lists/domains
          mv dist/ips.txt lists/ips
          mv dist/urls.txt lists/urls

      - name: Clean up temporary files
        run: |
          rm -f process_domains.py
          rm -rf dist/

      - name: Commit and push changes
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add lists/
          if ! git diff --cached --quiet; then
             git commit -m "Update Blocklist: $(date +'%Y-%m-%d %H:%M:%S')"
             git push
          else
            echo "No changes to commit."
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
