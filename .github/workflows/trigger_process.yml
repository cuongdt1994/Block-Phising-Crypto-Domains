name: Scheduled Script Execution

on:
  workflow_dispatch:
  schedule:
    - cron: "*/5 * * * *"

jobs:
  download-and-process:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Download and process all sources
        run: |
          cat << 'EOF' > process_domains.py
          import os
          import json
          import requests
          import re
          from urllib.parse import urlparse

          # Cloudflare DNS IPs to exclude
          EXCLUDED_IPS = {'1.1.1.1', '1.0.0.1'}

          # Domains to exclude
          EXCLUDED_DOMAINS = {
              'raw.githubusercontent.com',
              'gist.githubusercontent.com',
              's3.us-east-2.amazonaws.com',
              'cdn.discordapp.com'
          }

          def is_valid_domain(domain):
              """Validate domain format"""
              domain_pattern = r'^(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)*[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?$'
              return bool(re.match(domain_pattern, domain))

          def is_valid_ip(ip):
              """Validate IP format"""
              try:
                  parts = ip.split('.')
                  if len(parts) != 4:
                      return False
                  for part in parts:
                      if not part.isdigit():
                          return False
                      num = int(part)
                      if num < 0 or num > 255:
                          return False
                  return True
              except:
                  return False

          def extract_domain_from_url(url):
              """Extract domain from URL"""
              try:
                  parsed = urlparse(url)
                  return parsed.netloc
              except:
                  return None

          def process_ipsum_file(url):
              """Process IPsum file and extract IP addresses"""
              ips = []
              try:
                  print(f"Downloading from: {url}")
                  response = requests.get(url, timeout=30)
                  if response.status_code == 200:
                      lines = response.text.strip().split('\n')
                      for line in lines:
                          line = line.strip()
                          if line and not line.startswith('#') and not line.startswith('//'):
                              # IPsum files contain one IP per line
                              if is_valid_ip(line) and line not in EXCLUDED_IPS:
                                  ips.append(line)
                  else:
                      print(f"Failed to download {url}: HTTP {response.status_code}")
              except Exception as e:
                  print(f"Error processing {url}: {e}")
              
              return ips

          def remove_duplicates_and_report(items, item_type):
              """Remove duplicates from list and report statistics"""
              original_count = len(items)
              unique_items = list(set(items))
              duplicates_removed = original_count - len(unique_items)
              
              if duplicates_removed > 0:
                  print(f"Removed {duplicates_removed} duplicate {item_type}(s)")
              else:
                  print(f"No duplicates found in {item_type}s")
              
              return unique_items

          # List of IPsum sources (levels 1-8)
          ipsum_sources = [
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/1.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/2.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/3.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/4.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/5.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/6.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/7.txt',
              'https://raw.githubusercontent.com/stamparm/ipsum/refs/heads/master/levels/8.txt'
          ]

          # Create directories
          os.makedirs('dist', exist_ok=True)

          # Initialize lists for different types of indicators
          domains_list = []
          ips_list = []

          # Download and process JSON sources for domains
          print("=== PROCESSING JSON SOURCES FOR DOMAINS ===")
          
          # Download scamsniffer domains
          try:
              print("Processing scamsniffer domains...")
              scamsniffer_url = 'https://raw.githubusercontent.com/scamsniffer/scam-database/refs/heads/main/blacklist/domains.json'
              response = requests.get(scamsniffer_url, timeout=30)
              if response.status_code == 200:
                  scamsniffer_data = response.json()
                  for domain in scamsniffer_data:
                      if domain and domain not in EXCLUDED_DOMAINS and is_valid_domain(domain):
                          domains_list.append(domain)
                  print(f"Added {len([d for d in scamsniffer_data if d and d not in EXCLUDED_DOMAINS and is_valid_domain(d)])} domains from scamsniffer")
              else:
                  print(f"Failed to download scamsniffer: HTTP {response.status_code}")
          except Exception as e:
              print(f"Error downloading scamsniffer: {e}")

          # Download metamask blacklist
          try:
              print("Processing metamask blacklist...")
              metamask_url = 'https://raw.githubusercontent.com/MetaMask/eth-phishing-detect/refs/heads/main/src/config.json'
              response = requests.get(metamask_url, timeout=30)
              if response.status_code == 200:
                  metamask_data = response.json()
                  blacklist_items = metamask_data.get('blacklist', [])
                  for item in blacklist_items:
                      if item:
                          item = item.strip()
                          # If it's a URL, extract domain from it
                          if item.startswith('http://') or item.startswith('https://'):
                              domain = extract_domain_from_url(item)
                              if domain and is_valid_domain(domain) and domain not in EXCLUDED_DOMAINS:
                                  domains_list.append(domain)
                          # If it's a domain, add it directly
                          elif is_valid_domain(item) and item not in EXCLUDED_DOMAINS:
                              domains_list.append(item)
                  print(f"Processed {len(blacklist_items)} items from metamask")
              else:
                  print(f"Failed to download metamask: HTTP {response.status_code}")
          except Exception as e:
              print(f"Error downloading metamask: {e}")

          # Process IPsum sources for IPs
          print("=== PROCESSING IPSUM SOURCES FOR IPs ===")
          total_ipsum_ips = 0
          for i, source in enumerate(ipsum_sources):
              level = i + 1
              print(f"Processing IPsum level {level}...")
              try:
                  level_ips = process_ipsum_file(source)
                  ips_list.extend(level_ips)
                  total_ipsum_ips += len(level_ips)
                  print(f"Added {len(level_ips)} IPs from level {level}")
              except Exception as e:
                  print(f"Error processing level {level}: {e}")

          # Remove duplicates and get statistics
          print("=== REMOVING DUPLICATES ===")
          unique_domains = remove_duplicates_and_report(domains_list, "domain")
          unique_ips = remove_duplicates_and_report(ips_list, "IP")

          # Sort the unique lists
          unique_domains.sort()
          unique_ips.sort()

          # Write output files to dist directory
          print("=== WRITING OUTPUT FILES ===")
          
          # Write domains
          with open('dist/domains.txt', 'w') as f:
              for domain in unique_domains:
                  f.write(domain + '\n')
          print(f"Written {len(unique_domains)} unique domains to domains.txt")

          # Write IPs
          with open('dist/ips.txt', 'w') as f:
              for ip in unique_ips:
                  f.write(ip + '\n')
          print(f"Written {len(unique_ips)} unique IPs to ips.txt")

          print(f"=== PROCESSING COMPLETE ===")
          print(f"=== DUPLICATE REMOVAL SUMMARY ===")
          print(f"Domains: {len(domains_list)} → {len(unique_domains)} (removed {len(domains_list) - len(unique_domains)} duplicates)")
          print(f"IPs: {len(ips_list)} → {len(unique_ips)} (removed {len(ips_list) - len(unique_ips)} duplicates)")
          print(f"=== FINAL COUNTS ===")
          print(f"Total unique Domains: {len(unique_domains)}")
          print(f"Total unique IPs: {len(unique_ips)}")
          print(f"Excluded domains: {EXCLUDED_DOMAINS}")
          print(f"Excluded IPs: {EXCLUDED_IPS}")

          EOF
          python process_domains.py

      - name: Move output files to lists directory
        run: |
          mkdir -p lists
          mv dist/domains.txt lists/domains
          mv dist/ips.txt lists/ips

      - name: Clean up temporary files
        run: |
          rm -f process_domains.py
          rm -rf dist/

      - name: Commit and push changes
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add lists/
          if ! git diff --cached --quiet; then
             git commit -m "Update Blocklist: $(date +'%Y-%m-%d %H:%M:%S')"
             git push
          else
            echo "No changes to commit."
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
